{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "312783c9-6d69-46b8-95bb-7c99a5ceec75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Day 1: PROBLEM STATEMENT: Calculate the distance travelled by the individuals who started at the zero spot.\n",
    "\n",
    "Define the schema for the journey table\n",
    "schema = \"daycount INT, distancecovered INT\"\n",
    "\n",
    "Sample data\n",
    "data = [(1, 20), (2, 40), (3, 90), (4, 30), (5, 0), (6, 90), (7, 40), (8, 10), (9, 0), (10, 40), (11, 10), (12, 0)]\n",
    "\n",
    "Expected Output:\n",
    "TotalDistance 180 140 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fca8e6d-0f6e-4edd-bbfb-6c2d72f725bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from  pyspark.sql import functions as sf\n",
    "from  pyspark.sql import *\n",
    "from pyspark.sql.functions import sum, when\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Calculate distance travelled\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebe7367f-4f9b-4138-a437-1fdfb416deb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1, 20), (2, 40), (3, 90), (4, 30), (5, 0), (6, 90), (7, 40), (8, 10), (9, 0), (10, 40), (11, 10), (12, 0)]\n",
    "schema = \"daycount INT, distancecovered INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ef59da-2f61-40d5-8ff7-21333b6ebc89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "windowSpec=Window.orderBy(\"daycount\")\n",
    "df=df.withColumn(\"group\", sum(when(df.distancecovered==0,1).otherwise(0)).over(windowSpec.rowsBetween(Window.unboundedPreceding,0)))\n",
    "df1=df.groupBy(\"group\").agg(sum(\"distancecovered\").alias(\"Totaldistance\")).orderBy(\"group\")\n",
    "df1.filter(df1.Totaldistance>0).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38acefbf-e38a-40d9-a4a7-a938ba26246d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DAY 3 :PROBLEM STATEMENT: Look for only those companies whose revenue is rising annually. Let's say a company's revenue rises for three years, then drops the very following year. In that scenario, the revenue shouldn't be included in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37a7e07-50c6-4c63-b527-cfe229efaf86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "spark=SparkSession.builder.appName(\"Rising_Revenue\").getOrCreate()\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"company\",StringType(),True),\n",
    "    StructField(\"year\",IntegerType(),True),\n",
    "    StructField(\"revenue\",IntegerType(),True)])\n",
    "\n",
    "#Data to insert into company revenue taable \n",
    "\n",
    "data =[   ('ABC1', 2000, 100), \n",
    "    ('ABC1', 2001, 110), \n",
    "    ('ABC1', 2002, 120), \n",
    "    ('ABC2', 2000, 100), \n",
    "    ('ABC2', 2001, 90), \n",
    "    ('ABC2', 2002, 120), \n",
    "    ('ABC3', 2000, 500), \n",
    "    ('ABC3', 2001, 400), \n",
    "    ('ABC3', 2002, 600), \n",
    "    ('ABC3', 2003, 800) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5d0b2f-990c-48ae-af5c-be4d7b6d711a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n|company|year|revenue|\n+-------+----+-------+\n|   ABC1|2000|    100|\n|   ABC1|2001|    110|\n|   ABC1|2002|    120|\n|   ABC2|2000|    100|\n|   ABC2|2001|     90|\n|   ABC2|2002|    120|\n|   ABC3|2000|    500|\n|   ABC3|2001|    400|\n|   ABC3|2002|    600|\n|   ABC3|2003|    800|\n+-------+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "265d155c-2c5d-47ed-9568-0de663855113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a window partitioned by company and ordered by year\n",
    "windowSpec=Window.partitionBy('company').orderBy('year')\n",
    "\n",
    "# Calculate lagged revenue \n",
    "df=df.withColumn('prev_rev', lag('revenue').over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5648d62-622c-4b6d-91a1-f23ac675cf79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+--------+--------+\n|company|year|revenue|prev_rev|Rev_diff|\n+-------+----+-------+--------+--------+\n|   ABC1|2000|    100|    NULL|    NULL|\n|   ABC1|2001|    110|     100|      10|\n|   ABC1|2002|    120|     110|      10|\n|   ABC2|2000|    100|    NULL|    NULL|\n|   ABC2|2001|     90|     100|     -10|\n|   ABC2|2002|    120|      90|      30|\n|   ABC3|2000|    500|    NULL|    NULL|\n|   ABC3|2001|    400|     500|    -100|\n|   ABC3|2002|    600|     400|     200|\n|   ABC3|2003|    800|     600|     200|\n+-------+----+-------+--------+--------+\n\n+-------+\n|company|\n+-------+\n|   ABC1|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"Rev_diff\",df.revenue-df.prev_rev)\n",
    "df.show()\n",
    "result=df.groupBy('company').agg(F.min('Rev_diff').alias(\"Rev_Change\"))\n",
    "result1=result.filter(result.Rev_Change > 0 ).select('company').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76aa0f0c-ce84-4069-9d79-ed9f060b2a2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Hello, Connections ✨\n",
    "\n",
    "hashtag#DAY3\n",
    " \n",
    "Here is the hashtag#Day3 of Our hashtag#Pyspark, hashtag#SparkSQL Learning Challenge.\n",
    " \n",
    "Concepts for Data Engineer(PySpark, Spark SQL)\n",
    " \n",
    "𝐒𝐜𝐞𝐧𝐞𝐫𝐢𝐨 𝐰𝐢𝐭𝐡 𝐬𝐨𝐥𝐮𝐭𝐢𝐨𝐧\n",
    " \n",
    "𝐒𝐜𝐞𝐧𝐞𝐫𝐢𝐨:\n",
    "The daily reported COVID cases for 2021 are shown in the table. Determine the monthly percentage rise in Covid cases compared to the total number of cases as of the previous month. Sort the result by month and return the month number along with the percentage increase rounded to one decimal place. \n",
    "\n",
    "Try coming up with a new approach; I've included one in the post. Please double-check it.\n",
    " \n",
    "Regards! \n",
    "Together, we can grow and learn. \n",
    "Please share this again with your network.\n",
    " \n",
    "𝐅𝐨𝐥𝐥𝐨𝐰: https://lnkd.in/ggBSP5RF for more…\n",
    " \n",
    "Please check out our 𝕴𝖓𝖘𝖙𝖆𝖌𝖗𝖆𝖒 account for our latest Post: https://lnkd.in/gizfkVcy\n",
    "\n",
    "𝐏𝐫𝐨𝐣𝐞𝐜𝐭 𝐥𝐢𝐧𝐤: @https://lnkd.in/gxjKWsXj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45be7d3e-0b11-4afc-a4a9-bb446b37f8c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType,DateType,StringType,StructField,StructType\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "spark=SparkSession.builder.appName(\"Covid Percent Change\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe9676b-eb3f-4e01-86cd-d1bd00631919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n|record_date|cases_count|\n+-----------+-----------+\n| 2021-01-01|         66|\n| 2021-01-02|         41|\n| 2021-01-03|         54|\n| 2021-01-05|         16|\n| 2021-01-06|         90|\n| 2021-01-07|         34|\n| 2021-01-08|         84|\n| 2021-01-09|         71|\n| 2021-01-10|         14|\n| 2021-01-11|         48|\n| 2021-01-12|         72|\n| 2021-02-01|         38|\n| 2021-02-02|         57|\n| 2021-02-03|         42|\n| 2021-02-04|         61|\n| 2021-02-05|         25|\n| 2021-02-06|         78|\n| 2021-02-07|         33|\n| 2021-02-08|         93|\n| 2021-02-09|         62|\n+-----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data =[('2021-01-01',66),('2021-01-02',41),('2021-01-03',54), ('2021-01-05', 16), ('2021-01-06', 90), ('2021-01-07', 34), \n",
    "('2021-01-08', 84),  ('2021-01-09', 71), ('2021-01-10', 14), ('2021-01-11', 48), ('2021-01-12', 72), ('2021-02-01', 38), ('2021-02-02', 57), ('2021-02-03', 42), ('2021-02-04', 61), ('2021-02-05', 25), ('2021-02-06', 78), ('2021-02-07', 33), ('2021-02-08', 93), ('2021-02-09', 62), ('2021-02-10', 15), ('2021-02-11', 52), ('2021-02-12', 76)]\n",
    "\n",
    "schema=StructType([StructField(\"record_date\",StringType(),True),StructField(\"cases_count\",IntegerType(),True)])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82822fbc-3fe7-4003-bb00-7acf1dc65cad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('record_date', 'string'), ('cases_count', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3a8c14-5327-4fb5-815b-0f4cf4f8f472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n|record_date|cases_count|date_column|\n+-----------+-----------+-----------+\n| 2021-01-01|         66| 2021-01-01|\n| 2021-01-02|         41| 2021-01-02|\n| 2021-01-03|         54| 2021-01-03|\n| 2021-01-05|         16| 2021-01-05|\n| 2021-01-06|         90| 2021-01-06|\n| 2021-01-07|         34| 2021-01-07|\n| 2021-01-08|         84| 2021-01-08|\n| 2021-01-09|         71| 2021-01-09|\n| 2021-01-10|         14| 2021-01-10|\n| 2021-01-11|         48| 2021-01-11|\n| 2021-01-12|         72| 2021-01-12|\n| 2021-02-01|         38| 2021-02-01|\n| 2021-02-02|         57| 2021-02-02|\n| 2021-02-03|         42| 2021-02-03|\n| 2021-02-04|         61| 2021-02-04|\n| 2021-02-05|         25| 2021-02-05|\n| 2021-02-06|         78| 2021-02-06|\n| 2021-02-07|         33| 2021-02-07|\n| 2021-02-08|         93| 2021-02-08|\n| 2021-02-09|         62| 2021-02-09|\n+-----------+-----------+-----------+\nonly showing top 20 rows\n\n[('record_date', 'string'), ('cases_count', 'int'), ('date_column', 'date')]\n+------------+-----------+----+--------------+\n|record_month|total_cases|diff|percent_change|\n+------------+-----------+----+--------------+\n|           1|        590| 590|         100.0|\n|           2|        632|  42|          6.65|\n+------------+-----------+----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"date_column\",to_date(\"record_date\",\"yyyy-MM-dd\"))\n",
    "df.show()\n",
    "print(df.dtypes)\n",
    "cte_df=df.groupBy(month(\"date_column\").alias(\"record_month\")).agg(sum(\"cases_count\").alias(\"total_cases\"))\n",
    "#cte_df.show()\n",
    "cte_df2=cte_df.withColumn(\"prev_cases\", lag(\"total_cases\",1,0).over(Window.orderBy(\"record_month\"))).withColumn(\"diff\",col(\"total_cases\")- col(\"prev_cases\")).withColumn(\"percent_change\",round(100 *(col(\"diff\")/col(\"total_cases\")),2)).select(\"record_month\",\"total_cases\",\"diff\",\"percent_change\")\n",
    "cte_df2.show()\n",
    "#.rowsBetween(Window.unboundedPreceding,Window.currentRow))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ganesh_Pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
