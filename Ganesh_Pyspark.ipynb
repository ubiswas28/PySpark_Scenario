{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "312783c9-6d69-46b8-95bb-7c99a5ceec75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Day 1: PROBLEM STATEMENT: Calculate the distance travelled by the individuals who started at the zero spot.\n",
    "\n",
    "Define the schema for the journey table\n",
    "schema = \"daycount INT, distancecovered INT\"\n",
    "\n",
    "Sample data\n",
    "data = [(1, 20), (2, 40), (3, 90), (4, 30), (5, 0), (6, 90), (7, 40), (8, 10), (9, 0), (10, 40), (11, 10), (12, 0)]\n",
    "\n",
    "Expected Output:\n",
    "TotalDistance 180 140 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fca8e6d-0f6e-4edd-bbfb-6c2d72f725bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from  pyspark.sql import functions as sf\n",
    "from  pyspark.sql import *\n",
    "from pyspark.sql.functions import sum, when\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Calculate distance travelled\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebe7367f-4f9b-4138-a437-1fdfb416deb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1, 20), (2, 40), (3, 90), (4, 30), (5, 0), (6, 90), (7, 40), (8, 10), (9, 0), (10, 40), (11, 10), (12, 0)]\n",
    "schema = \"daycount INT, distancecovered INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ef59da-2f61-40d5-8ff7-21333b6ebc89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "windowSpec=Window.orderBy(\"daycount\")\n",
    "df=df.withColumn(\"group\", sum(when(df.distancecovered==0,1).otherwise(0)).over(windowSpec.rowsBetween(Window.unboundedPreceding,0)))\n",
    "df1=df.groupBy(\"group\").agg(sum(\"distancecovered\").alias(\"Totaldistance\")).orderBy(\"group\")\n",
    "df1.filter(df1.Totaldistance>0).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38acefbf-e38a-40d9-a4a7-a938ba26246d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DAY 3 :PROBLEM STATEMENT: Look for only those companies whose revenue is rising annually. Let's say a company's revenue rises for three years, then drops the very following year. In that scenario, the revenue shouldn't be included in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37a7e07-50c6-4c63-b527-cfe229efaf86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "spark=SparkSession.builder.appName(\"Rising_Revenue\").getOrCreate()\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"company\",StringType(),True),\n",
    "    StructField(\"year\",IntegerType(),True),\n",
    "    StructField(\"revenue\",IntegerType(),True)])\n",
    "\n",
    "#Data to insert into company revenue taable \n",
    "\n",
    "data =[   ('ABC1', 2000, 100), \n",
    "    ('ABC1', 2001, 110), \n",
    "    ('ABC1', 2002, 120), \n",
    "    ('ABC2', 2000, 100), \n",
    "    ('ABC2', 2001, 90), \n",
    "    ('ABC2', 2002, 120), \n",
    "    ('ABC3', 2000, 500), \n",
    "    ('ABC3', 2001, 400), \n",
    "    ('ABC3', 2002, 600), \n",
    "    ('ABC3', 2003, 800) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5d0b2f-990c-48ae-af5c-be4d7b6d711a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n|company|year|revenue|\n+-------+----+-------+\n|   ABC1|2000|    100|\n|   ABC1|2001|    110|\n|   ABC1|2002|    120|\n|   ABC2|2000|    100|\n|   ABC2|2001|     90|\n|   ABC2|2002|    120|\n|   ABC3|2000|    500|\n|   ABC3|2001|    400|\n|   ABC3|2002|    600|\n|   ABC3|2003|    800|\n+-------+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "265d155c-2c5d-47ed-9568-0de663855113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a window partitioned by company and ordered by year\n",
    "windowSpec=Window.partitionBy('company').orderBy('year')\n",
    "\n",
    "# Calculate lagged revenue \n",
    "df=df.withColumn('prev_rev', lag('revenue').over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5648d62-622c-4b6d-91a1-f23ac675cf79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+--------+--------+\n|company|year|revenue|prev_rev|Rev_diff|\n+-------+----+-------+--------+--------+\n|   ABC1|2000|    100|    NULL|    NULL|\n|   ABC1|2001|    110|     100|      10|\n|   ABC1|2002|    120|     110|      10|\n|   ABC2|2000|    100|    NULL|    NULL|\n|   ABC2|2001|     90|     100|     -10|\n|   ABC2|2002|    120|      90|      30|\n|   ABC3|2000|    500|    NULL|    NULL|\n|   ABC3|2001|    400|     500|    -100|\n|   ABC3|2002|    600|     400|     200|\n|   ABC3|2003|    800|     600|     200|\n+-------+----+-------+--------+--------+\n\n+-------+\n|company|\n+-------+\n|   ABC1|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"Rev_diff\",df.revenue-df.prev_rev)\n",
    "df.show()\n",
    "result=df.groupBy('company').agg(F.min('Rev_diff').alias(\"Rev_Change\"))\n",
    "result1=result.filter(result.Rev_Change > 0 ).select('company').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76aa0f0c-ce84-4069-9d79-ed9f060b2a2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Hello, Connections âœ¨\n",
    "\n",
    "hashtag#DAY3\n",
    " \n",
    "Here is the hashtag#Day3 of Our hashtag#Pyspark, hashtag#SparkSQL Learning Challenge.\n",
    " \n",
    "Concepts for Data Engineer(PySpark, Spark SQL)\n",
    " \n",
    "ğ’ğœğğ§ğğ«ğ¢ğ¨ ğ°ğ¢ğ­ğ¡ ğ¬ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§\n",
    " \n",
    "ğ’ğœğğ§ğğ«ğ¢ğ¨:\n",
    "The daily reported COVID cases for 2021 are shown in the table. Determine the monthly percentage rise in Covid cases compared to the total number of cases as of the previous month. Sort the result by month and return the month number along with the percentage increase rounded to one decimal place. \n",
    "\n",
    "Try coming up with a new approach; I've included one in the post. Please double-check it.\n",
    " \n",
    "Regards! \n",
    "Together, we can grow and learn. \n",
    "Please share this again with your network.\n",
    " \n",
    "ğ…ğ¨ğ¥ğ¥ğ¨ğ°: https://lnkd.in/ggBSP5RF for moreâ€¦\n",
    " \n",
    "Please check out our ğ•´ğ–“ğ–˜ğ–™ğ–†ğ–Œğ–—ğ–†ğ–’ account for our latest Post: https://lnkd.in/gizfkVcy\n",
    "\n",
    "ğğ«ğ¨ğ£ğğœğ­ ğ¥ğ¢ğ§ğ¤: @https://lnkd.in/gxjKWsXj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45be7d3e-0b11-4afc-a4a9-bb446b37f8c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType,DateType,StringType,StructField,StructType\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "spark=SparkSession.builder.appName(\"Covid Percent Change\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe9676b-eb3f-4e01-86cd-d1bd00631919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n|record_date|cases_count|\n+-----------+-----------+\n| 2021-01-01|         66|\n| 2021-01-02|         41|\n| 2021-01-03|         54|\n| 2021-01-05|         16|\n| 2021-01-06|         90|\n| 2021-01-07|         34|\n| 2021-01-08|         84|\n| 2021-01-09|         71|\n| 2021-01-10|         14|\n| 2021-01-11|         48|\n| 2021-01-12|         72|\n| 2021-02-01|         38|\n| 2021-02-02|         57|\n| 2021-02-03|         42|\n| 2021-02-04|         61|\n| 2021-02-05|         25|\n| 2021-02-06|         78|\n| 2021-02-07|         33|\n| 2021-02-08|         93|\n| 2021-02-09|         62|\n+-----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data =[('2021-01-01',66),('2021-01-02',41),('2021-01-03',54), ('2021-01-05', 16), ('2021-01-06', 90), ('2021-01-07', 34), \n",
    "('2021-01-08', 84),  ('2021-01-09', 71), ('2021-01-10', 14), ('2021-01-11', 48), ('2021-01-12', 72), ('2021-02-01', 38), ('2021-02-02', 57), ('2021-02-03', 42), ('2021-02-04', 61), ('2021-02-05', 25), ('2021-02-06', 78), ('2021-02-07', 33), ('2021-02-08', 93), ('2021-02-09', 62), ('2021-02-10', 15), ('2021-02-11', 52), ('2021-02-12', 76)]\n",
    "\n",
    "schema=StructType([StructField(\"record_date\",StringType(),True),StructField(\"cases_count\",IntegerType(),True)])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82822fbc-3fe7-4003-bb00-7acf1dc65cad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('record_date', 'string'), ('cases_count', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc3a8c14-5327-4fb5-815b-0f4cf4f8f472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n|record_date|cases_count|date_column|\n+-----------+-----------+-----------+\n| 2021-01-01|         66| 2021-01-01|\n| 2021-01-02|         41| 2021-01-02|\n| 2021-01-03|         54| 2021-01-03|\n| 2021-01-05|         16| 2021-01-05|\n| 2021-01-06|         90| 2021-01-06|\n| 2021-01-07|         34| 2021-01-07|\n| 2021-01-08|         84| 2021-01-08|\n| 2021-01-09|         71| 2021-01-09|\n| 2021-01-10|         14| 2021-01-10|\n| 2021-01-11|         48| 2021-01-11|\n| 2021-01-12|         72| 2021-01-12|\n| 2021-02-01|         38| 2021-02-01|\n| 2021-02-02|         57| 2021-02-02|\n| 2021-02-03|         42| 2021-02-03|\n| 2021-02-04|         61| 2021-02-04|\n| 2021-02-05|         25| 2021-02-05|\n| 2021-02-06|         78| 2021-02-06|\n| 2021-02-07|         33| 2021-02-07|\n| 2021-02-08|         93| 2021-02-08|\n| 2021-02-09|         62| 2021-02-09|\n+-----------+-----------+-----------+\nonly showing top 20 rows\n\n[('record_date', 'string'), ('cases_count', 'int'), ('date_column', 'date')]\n+------------+-----------+----+--------------+\n|record_month|total_cases|diff|percent_change|\n+------------+-----------+----+--------------+\n|           1|        590| 590|         100.0|\n|           2|        632|  42|          6.65|\n+------------+-----------+----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"date_column\",to_date(\"record_date\",\"yyyy-MM-dd\"))\n",
    "df.show()\n",
    "print(df.dtypes)\n",
    "cte_df=df.groupBy(month(\"date_column\").alias(\"record_month\")).agg(sum(\"cases_count\").alias(\"total_cases\"))\n",
    "#cte_df.show()\n",
    "cte_df2=cte_df.withColumn(\"prev_cases\", lag(\"total_cases\",1,0).over(Window.orderBy(\"record_month\"))).withColumn(\"diff\",col(\"total_cases\")- col(\"prev_cases\")).withColumn(\"percent_change\",round(100 *(col(\"diff\")/col(\"total_cases\")),2)).select(\"record_month\",\"total_cases\",\"diff\",\"percent_change\")\n",
    "cte_df2.show()\n",
    "#.rowsBetween(Window.unboundedPreceding,Window.currentRow))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ganesh_Pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
