Topic 17:
Interview Question: Explain ways to handle null values in your code.

Handling null values is a common task in PySpark, and there are various ways to deal with them depending on the use case. Here, I'll explain some common methods for handling nulls in PySpark along with code examples.

Code:
from pyspark.sql.functions import col, avg , when

data = [(1, "Alice", 25),
 (2, "Bob", None),
 (3, None, 30)]

schema = ["id", "name", "age"]
df = spark.createDataFrame(data, schema=schema)
hashtag

#dropping
df_no_nulls = df.dropna()
df_no_nulls.show()

#filling
df_filled = df.fillna(0, subset=["age"])
df_filled.show()


df_filled_2 = df.withColumn("age", when(col("age").isNull(), 0).otherwise(col("age")))
df_filled_2.show()

avg_age = df.select(avg("age")).first()[0]
df_filled_3 = df.withColumn("age",when(col("age").isNull(), avg_age).otherwise(col("age")) )
df_filled_3.show()
