{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba14857d-87c6-4e62-843a-86abc5207663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|age| Name|\n+---+-----+\n|  2|Alice|\n|  5|  Bob|\n+---+-----+\n\n+--------+\n|min(age)|\n+--------+\n|       2|\n+--------+\n\n+--------+\n|max(age)|\n+--------+\n|       5|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "spark=SparkSession.builder.appName(\"aggregate\").getOrCreate()\n",
    "\n",
    "data=[[2,\"Alice\"],[5,\"Bob\"]]\n",
    "schema=(\"age INT, Name STRING\")\n",
    "df=spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "df_1=df.agg(sf.min(df.age))\n",
    "df_2=df.agg({\"age\":\"max\"})\n",
    "df_1.show()\n",
    "df_2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c262d3ed-30e2-46a7-85f3-8b052ec686bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e3229f-aa7b-4b20-b3d6-8c0cf7da3dba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Running Sum in window function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b252b563-996e-43ec-9713-43746f7a7ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n| id| name|score|\n+---+-----+-----+\n|  1|Alice|   80|\n|  2|  Bob|   90|\n|  3|Alice|   85|\n|  4|  Bob|   70|\n|  5|Carol|   75|\n+---+-----+-----+\n\n+---+-----+-------------+\n| id|score|running_total|\n+---+-----+-------------+\n|  1|   80|         80.0|\n|  2|   90|        170.0|\n|  3|   85|        255.0|\n|  4|   70|        325.0|\n|  5|   75|        400.0|\n+---+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "#from pyspark.sql.functions import col, sum\n",
    "spark=SparkSession.builder.appName(\"running_total\").getOrCreate()\n",
    "\n",
    "\n",
    "data=[[\"1\",\"Alice\",\"80\"],\n",
    "      [\"2\",\"Bob\",\"90\"],\n",
    "      [\"3\",\"Alice\",\"85\"],\n",
    "      [\"4\",\"Bob\",\"70\"],\n",
    "      [\"5\",\"Carol\",\"75\"]]\n",
    "schema=(\"id string, name String, score string\")\n",
    "\n",
    "df= spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "windowSpec=Window.orderBy(\"id\")\n",
    "df.select(\"id\",\"score\",sum(col(\"score\")).over(windowSpec).alias(\"running_total\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b06ac76-70a7-4622-bc43-6a03446b7368",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n| id| name|score|\n+---+-----+-----+\n|  1|Alice|   80|\n|  2|  Bob|   90|\n|  3|Alice|   85|\n|  4|  Bob|   70|\n|  5|Carol|   75|\n+---+-----+-----+\n\n+---+-----+-------------+\n| id|score|running_total|\n+---+-----+-------------+\n|  1|   80|           80|\n|  2|   90|          170|\n|  3|   85|          255|\n|  4|   70|          325|\n|  5|   75|          400|\n+---+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"running_total\").getOrCreate()\n",
    "\n",
    "\n",
    "data=[[1,\"Alice\",80],\n",
    "      [2,\"Bob\",90],\n",
    "      [3,\"Alice\",85],\n",
    "      [4,\"Bob\",70],\n",
    "      [5,\"Carol\",75]]\n",
    "schema=(\"id int, name String, score int\")\n",
    "\n",
    "df= spark.createDataFrame(data,schema=schema)\n",
    "df.show()\n",
    "windowSpec=Window.orderBy(\"id\")\n",
    "df.select(\"id\",\"score\",sum(\"score\").over(windowSpec).alias(\"running_total\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0685efa-21ac-4a4b-bac5-611019e2f4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Distinct and Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cad158d-d3bb-4df0-88e4-d2aa45caebb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Distinct\").getOrCreate()\n",
    "\n",
    "data=[[14,\"Tom\"],\n",
    "      [23,\"Alice\"],\n",
    "      [23,\"Alice\"]]\n",
    "schema=\"age INT, name STRING\"\n",
    "df=spark.createDataFrame(data,schema=schema).distinct()\n",
    "df.count() #better to count in next line#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e854d49a-8845-4178-9d7e-d88eee54dc17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|age| name|\n+---+-----+\n| 14|  Tom|\n| 23|Alice|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "797809e6-ce80-4307-b9c6-66cff6dfbb9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "                                                           DROP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36eefa98-d3a7-4a21-8adb-9e9bc61361b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|age|\n+---+\n| 14|\n| 23|\n| 16|\n+---+\n\n+-----+\n| name|\n+-----+\n|  Tom|\n|Alice|\n|  Bob|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark \n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "spark=SparkSession.builder.appName(\"Drop\").getOrCreate()\n",
    "\n",
    "data=[[14,\"Tom\"],\n",
    "[23,\"Alice\"],\n",
    "[16,\"Bob\"]]\n",
    "\n",
    "schema=\"age INT, name STRING\"\n",
    "\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df2=spark.createDataFrame([Row(height=80,name=\"Tom\"),Row(height=85,name=\"Bob\")])\n",
    "\n",
    "df.drop('name').show()\n",
    "\n",
    "df.drop(df.age).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e8cb67-41e2-46c9-b61d-9b3fac48f023",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1195613495446551>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#drop the colun that joined both dataframe \u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m df\u001B[38;5;241m.\u001B[39mjoin(df2,df\u001B[38;5;241m.\u001B[39mname\u001B[38;5;241m==\u001B[39mdf2\u001B[38;5;241m.\u001B[39mname,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39msort(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Error AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:162\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:224\u001B[0m, in \u001B[0;36mcol\u001B[0;34m(col)\u001B[0m\n",
       "\u001B[1;32m    197\u001B[0m \u001B[38;5;129m@try_remote_functions\u001B[39m\n",
       "\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcol\u001B[39m(col: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n",
       "\u001B[1;32m    199\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.\u001B[39;00m\n",
       "\u001B[1;32m    201\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    222\u001B[0m \u001B[38;5;124;03m    Column<'x'>\u001B[39;00m\n",
       "\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcol\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:98\u001B[0m, in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n",
       "\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m     97\u001B[0m jf \u001B[38;5;241m=\u001B[39m _get_jvm_function(name, SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context)\n",
       "\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(\u001B[43mjf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1347\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n",
       "\u001B[0;32m-> 1347\u001B[0m     args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1349\u001B[0m     command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m         args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m         proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1310\u001B[0m, in \u001B[0;36mJavaMember._build_args\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1308\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_build_args\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n",
       "\u001B[1;32m   1309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconverters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconverters) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[0;32m-> 1310\u001B[0m         (new_args, temp_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1311\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1312\u001B[0m         new_args \u001B[38;5;241m=\u001B[39m args\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1297\u001B[0m, in \u001B[0;36mJavaMember._get_args\u001B[0;34m(self, args)\u001B[0m\n",
       "\u001B[1;32m   1295\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m converter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39mconverters:\n",
       "\u001B[1;32m   1296\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m converter\u001B[38;5;241m.\u001B[39mcan_convert(arg):\n",
       "\u001B[0;32m-> 1297\u001B[0m         temp_arg \u001B[38;5;241m=\u001B[39m \u001B[43mconverter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1298\u001B[0m         temp_args\u001B[38;5;241m.\u001B[39mappend(temp_arg)\n",
       "\u001B[1;32m   1299\u001B[0m         new_args\u001B[38;5;241m.\u001B[39mappend(temp_arg)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_collections.py:510\u001B[0m, in \u001B[0;36mListConverter.convert\u001B[0;34m(self, object, gateway_client)\u001B[0m\n",
       "\u001B[1;32m    508\u001B[0m ArrayList \u001B[38;5;241m=\u001B[39m JavaClass(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjava.util.ArrayList\u001B[39m\u001B[38;5;124m\"\u001B[39m, gateway_client)\n",
       "\u001B[1;32m    509\u001B[0m java_list \u001B[38;5;241m=\u001B[39m ArrayList()\n",
       "\u001B[0;32m--> 510\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mobject\u001B[39m:\n",
       "\u001B[1;32m    511\u001B[0m     java_list\u001B[38;5;241m.\u001B[39madd(element)\n",
       "\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m java_list\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:725\u001B[0m, in \u001B[0;36mColumn.__iter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 725\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    726\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_ITERABLE\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjectName\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n",
       "\u001B[1;32m    727\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_ITERABLE] Column is not iterable."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\nFile \u001B[0;32m<command-1195613495446551>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#drop the colun that joined both dataframe \u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df\u001B[38;5;241m.\u001B[39mjoin(df2,df\u001B[38;5;241m.\u001B[39mname\u001B[38;5;241m==\u001B[39mdf2\u001B[38;5;241m.\u001B[39mname,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39msort(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Error AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:162\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:224\u001B[0m, in \u001B[0;36mcol\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;129m@try_remote_functions\u001B[39m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcol\u001B[39m(col: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m    Returns a :class:`~pyspark.sql.Column` based on the given column name.\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;124;03m    Column<'x'>\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcol\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:98\u001B[0m, in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     97\u001B[0m jf \u001B[38;5;241m=\u001B[39m _get_jvm_function(name, SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context)\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(\u001B[43mjf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1347\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m-> 1347\u001B[0m     args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1349\u001B[0m     command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m         args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m         proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1310\u001B[0m, in \u001B[0;36mJavaMember._build_args\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_build_args\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m   1309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconverters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconverters) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1310\u001B[0m         (new_args, temp_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1311\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1312\u001B[0m         new_args \u001B[38;5;241m=\u001B[39m args\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1297\u001B[0m, in \u001B[0;36mJavaMember._get_args\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m   1295\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m converter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39mconverters:\n\u001B[1;32m   1296\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m converter\u001B[38;5;241m.\u001B[39mcan_convert(arg):\n\u001B[0;32m-> 1297\u001B[0m         temp_arg \u001B[38;5;241m=\u001B[39m \u001B[43mconverter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1298\u001B[0m         temp_args\u001B[38;5;241m.\u001B[39mappend(temp_arg)\n\u001B[1;32m   1299\u001B[0m         new_args\u001B[38;5;241m.\u001B[39mappend(temp_arg)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_collections.py:510\u001B[0m, in \u001B[0;36mListConverter.convert\u001B[0;34m(self, object, gateway_client)\u001B[0m\n\u001B[1;32m    508\u001B[0m ArrayList \u001B[38;5;241m=\u001B[39m JavaClass(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjava.util.ArrayList\u001B[39m\u001B[38;5;124m\"\u001B[39m, gateway_client)\n\u001B[1;32m    509\u001B[0m java_list \u001B[38;5;241m=\u001B[39m ArrayList()\n\u001B[0;32m--> 510\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mobject\u001B[39m:\n\u001B[1;32m    511\u001B[0m     java_list\u001B[38;5;241m.\u001B[39madd(element)\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m java_list\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:725\u001B[0m, in \u001B[0;36mColumn.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 725\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    726\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_ITERABLE\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjectName\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[1;32m    727\u001B[0m     )\n\n\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_ITERABLE] Column is not iterable.",
       "errorSummary": "<span class='ansi-red-fg'>PySparkTypeError</span>: [NOT_ITERABLE] Column is not iterable.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cannot drop col(‘name’) due to ambiguous reference.\n",
    "\n",
    "df.join(df2,df.name==df2.name,'inner').drop(col(df.name)).sort('age').show()\n",
    "# Error AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897db146-4891-461c-8c8f-25b1deff4f63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n|age|height|\n+---+------+\n| 14|    80|\n| 16|    85|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#drop the colun that joined both dataframe \n",
    "\n",
    "df.join(df2,df.name==df2.name,'inner').drop('name').sort('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68402a1b-6fe9-49b8-8fcc-0ad93d843f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n|age| name|a.b.c|\n+---+-----+-----+\n| 14|  Tom|    1|\n| 23|Alice|    1|\n| 16|  Bob|    1|\n+---+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df4=df.withColumn('a.b.c',lit(1))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d90810-328a-46fd-952e-059428b4a29d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|age| name|\n+---+-----+\n| 14|  Tom|\n| 23|Alice|\n| 16|  Bob|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df4.drop('a.b.c').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6f5363-fd8d-41ca-9674-32444f5f1939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n|age| name|a.b.c|\n+---+-----+-----+\n| 14|  Tom|    1|\n| 23|Alice|    1|\n| 16|  Bob|    1|\n+---+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Cannot find a column matching the expression “a.b.c”.\n",
    "df4.drop(col(\"a.b.c\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc393d66-af67-470f-a32c-26fc5e1ea338",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "                                          DROP DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "276e9fcd-e28c-40f9-aa06-f39f361697a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c61eeb-26ab-4014-ac42-b8f6920f6d6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n| name|age|height|\n+-----+---+------+\n|Alice|  5|    80|\n|Alice|  5|    80|\n|Alice| 10|    80|\n+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from  pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=spark.createDataFrame([\n",
    "Row(name='Alice',age=5,height=80),\n",
    "Row(name='Alice',age=5,height=80),\n",
    "Row(name='Alice',age=10,height=80)\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e961fae3-aa1d-4458-90df-6e9b83c4dc35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n| name|age|height|\n+-----+---+------+\n|Alice|  5|    80|\n|Alice| 10|    80|\n+-----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['age','height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24149f48-2bc6-4f56-9818-6024c32781da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Transformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
